{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35793412-a0e4-4eea-be5d-7495a18429d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# \uD83C\uDFC6 IFCO Data Engineering Challenge\n",
    "**Medallion Architecture implemented in PySpark**\n",
    "\n",
    "This notebook solves the technical challenge by applying Data Engineering best practices. The processing is divided into three logical layers (Bronze, Silver and Gold) to ensure traceability, cleansing and data quality, using **Test-Driven Development (TDD)** methodologies for the required validations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac8cd5c3-2d3d-4ed9-8ff9-f447cb8783d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# \uD83E\uDD49 1. Bronze Layer (Raw Data)\n",
    "**Objective:** Ingest the raw data (`orders.csv` and `invoicing_data.json`) into the Databricks environment with minimal alteration.\n",
    "\n",
    "**Actions performed:**\n",
    "* Reading local files while handling cluster security restrictions using Pandas on the *Driver* node.\n",
    "* Chossing the right separator (`;`) in the orders CSV.\n",
    "* Automatic sanitization of column names to comply with **Delta Lake** format restrictions.\n",
    "* Initial storage of the data exactly as received in tabular format (`default.orders` and `default.invoicing_data`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65640ef3-3686-4193-b42c-0184748701ff",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Clean up"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\uD83E\uDDF9 Cleaning 'default' catalog to ensure a fresh Medallion execution...\n   Dropping managed: bronze_invoicing\n   Dropping managed: bronze_orders\n   Dropping managed: gold_companies_salesowners\n   Dropping managed: gold_sales_commissions\n   Dropping managed: silver_invoicing\n   Dropping managed: silver_orders\n✨ Catalog is now empty. Ready for Bronze, Silver, and Gold processing.\n\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 0. CATALOG CLEANUP (Environment Reset)\n",
    "# ==========================================\n",
    "print(\"\uD83E\uDDF9 Cleaning 'default' catalog to ensure a fresh Medallion execution...\")\n",
    "\n",
    "# Retrieve the list of all tables and views in the default database\n",
    "try:\n",
    "    tables = spark.catalog.listTables(\"default\")\n",
    "\n",
    "    for table in tables:\n",
    "        # Dynamically drop each table or view\n",
    "        print(f\"   Dropping {table.tableType.lower()}: {table.name}\")\n",
    "        spark.sql(f\"DROP TABLE IF EXISTS default.{table.name}\")\n",
    "        \n",
    "        # Explicitly drop views just in case they are listed differently\n",
    "        spark.sql(f\"DROP VIEW IF EXISTS default.{table.name}\")\n",
    "\n",
    "    print(\"✨ Catalog is now empty. Ready for Bronze, Silver, and Gold processing.\\n\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Note: Could not clear catalog (might be empty or missing permissions). Details: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5e13a18-17fe-4025-b3d5-08dc2f737918",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Bronze Ingestion"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/4 Reading files with Pandas...\n2/4 Converting to Spark...\n3/4 Sanitizing names...\n4/4 Saving Bronze tables...\nBronze Layer completed!\n\nTable: default.bronze_orders\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>order_id</th><th>date</th><th>company_id</th><th>company_name</th><th>crate_type</th><th>contact_data</th><th>salesowners</th></tr></thead><tbody><tr><td>f47ac10b-58cc-4372-a567-0e02b2c3d479</td><td>29.01.22</td><td>1e2b47e6-499e-41c6-91d3-09d12dddfbbd</td><td>Fresh Fruits Co</td><td>Plastic</td><td>[{ \"contact_name\":\"Curtis\", \"contact_surname\":\"Jackson\", \"city\":\"Chicago\", \"cp\": \"12345\"}]</td><td>Leonard Cohen, Luke Skywalker, Ammy Winehouse</td></tr><tr><td>f47ac10b-58cc-4372-a567-0e02b2c3d480</td><td>21.02.22</td><td>0f05a8f1-2bdf-4be7-8c82-4c9b58f04898</td><td>Veggies Inc</td><td>Wood</td><td>[{ \"contact_name\":\"Maria\", \"contact_surname\":\"Theresa\", \"city\":\"Calcutta\"}]</td><td>Luke Skywalker, David Goliat, Leon Leonov</td></tr><tr><td>f47ac10b-58cc-4372-a567-0e02b2c3d481</td><td>03.04.22</td><td>1e2b47e6-499e-41c6-91d3-09d12dddfbbd</td><td>Fresh Fruits c.o</td><td>Metal</td><td>[{ \"contact_name\":\"Para\", \"contact_surname\":\"Cetamol\", \"city\":\"Frankfurt am Oder\", \"cp\": 3934}]</td><td>Luke Skywalker</td></tr><tr><td>f47ac10b-58cc-4372-a567-0e02b2c3d482</td><td>14.07.21</td><td>1c4b0b50-1d5d-463a-b56e-1a6fd3aeb7d6</td><td>Seafood Supplier</td><td>Plastic</td><td>null</td><td>David Goliat, Leonard Cohen</td></tr><tr><td>f47ac10b-58cc-4372-a567-0e02b2c3d483</td><td>23.10.22</td><td>34538e39-cd2e-4641-8d24-3c94146e6f16</td><td>Meat Packers Ltd</td><td>Plastic</td><td>null</td><td>Chris Pratt, David Henderson, Marianov Merschik, Leon Leonov</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "f47ac10b-58cc-4372-a567-0e02b2c3d479",
         "29.01.22",
         "1e2b47e6-499e-41c6-91d3-09d12dddfbbd",
         "Fresh Fruits Co",
         "Plastic",
         "[{ \"contact_name\":\"Curtis\", \"contact_surname\":\"Jackson\", \"city\":\"Chicago\", \"cp\": \"12345\"}]",
         "Leonard Cohen, Luke Skywalker, Ammy Winehouse"
        ],
        [
         "f47ac10b-58cc-4372-a567-0e02b2c3d480",
         "21.02.22",
         "0f05a8f1-2bdf-4be7-8c82-4c9b58f04898",
         "Veggies Inc",
         "Wood",
         "[{ \"contact_name\":\"Maria\", \"contact_surname\":\"Theresa\", \"city\":\"Calcutta\"}]",
         "Luke Skywalker, David Goliat, Leon Leonov"
        ],
        [
         "f47ac10b-58cc-4372-a567-0e02b2c3d481",
         "03.04.22",
         "1e2b47e6-499e-41c6-91d3-09d12dddfbbd",
         "Fresh Fruits c.o",
         "Metal",
         "[{ \"contact_name\":\"Para\", \"contact_surname\":\"Cetamol\", \"city\":\"Frankfurt am Oder\", \"cp\": 3934}]",
         "Luke Skywalker"
        ],
        [
         "f47ac10b-58cc-4372-a567-0e02b2c3d482",
         "14.07.21",
         "1c4b0b50-1d5d-463a-b56e-1a6fd3aeb7d6",
         "Seafood Supplier",
         "Plastic",
         null,
         "David Goliat, Leonard Cohen"
        ],
        [
         "f47ac10b-58cc-4372-a567-0e02b2c3d483",
         "23.10.22",
         "34538e39-cd2e-4641-8d24-3c94146e6f16",
         "Meat Packers Ltd",
         "Plastic",
         null,
         "Chris Pratt, David Henderson, Marianov Merschik, Leon Leonov"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "order_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "date",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "company_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "company_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "crate_type",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "contact_data",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "salesowners",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nTable: default.bronze_invoicing\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>data</th></tr></thead><tbody><tr><td>List(List(1e2b47e6-499e-41c6-91d3-09d12dddfbbd, 324222, e1e1e1e1-e1e1-e1e1-e1e1-e1e1e1e1e1e1, f47ac10b-58cc-4372-a567-0e02b2c3d479, 0), List(0f05a8f1-2bdf-4be7-8c82-4c9b58f04898, 193498, e2e2e2e2-e2e2-e2e2-e2e2-e2e2e2e2e2e2, f47ac10b-58cc-4372-a567-0e02b2c3d480, 19), List(1e2b47e6-499e-41c6-91d3-09d12dddfbbd, 345498, e3e3e3e3-e3e3-e3e3-e3e3-e3e3e3e3e3e3, f47ac10b-58cc-4372-a567-0e02b2c3d481, 21), List(1c4b0b50-1d5d-463a-b56e-1a6fd3aeb7d6, 245412, e4e4e4e4-e4e4-e4e4-e4e4-e4e4e4e4e4e4, f47ac10b-58cc-4372-a567-0e02b2c3d482, 34), List(34538e39-cd2e-4641-8d24-3c94146e6f16, 145467, e5e5e5e5-e5e5-e5e5-e5e5-e5e5e5e5e5e5, f47ac10b-58cc-4372-a567-0e02b2c3d483, 0), List(fa14c3ed-3c48-49f4-bd69-4d7f5b5f4b1b, 581530, e6e6e6e6-e6e6-e6e6-e6e6-e6e6e6e6e6e6, f47ac10b-58cc-4372-a567-0e02b2c3d484, 19), List(2e90f2b1-d237-47a6-96e8-6d01c0d78c3e, 45100, e7e7e7e7-e7e7-e7e7-e7e7-e7e7e7e7e7e7, f47ac10b-58cc-4372-a567-0e02b2c3d485, 19), List(acdb6f30-764f-404e-8b8e-7e7e3e6fa1a9, 565210, e8e8e8e8-e8e8-e8e8-e8e8-e8e8e8e8e8e8, f47ac10b-58cc-4372-a567-0e02b2c3d486, 21), List(5f0bdbdf-1d84-4c23-957c-8bb8c0ddc89d, 345310, e9e9e9e9-e9e9-e9e9-e9e9-e9e9e9e9e9e9, f47ac10b-58cc-4372-a567-0e02b2c3d487, 34), List(5f0bdbdf-1d84-4c23-957c-8bb8c0ddc89d, 345310, ea9ea9ea-9ea9-9ea9-9ea9-9ea9ea9ea9ea, f47ac10b-58cc-4372-a567-0e02b2c3d487, 34), List(27c59f76-5d26-4b82-a89b-59f8dfd2e9a7, 341315, eb0eb0eb-0eb0-0eb0-0eb0-0eb0eb0eb0eb, f47ac10b-58cc-4372-a567-0e02b2c3d488, 21), List(20dfef10-8f4e-45a1-82fc-123f4ab2a4a5, 291315, ec1ec1ec-1ec1-1ec1-1ec1-1ec1ec1ec1ec, f47ac10b-58cc-4372-a567-0e02b2c3d489, 0))</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         [
          [
           "1e2b47e6-499e-41c6-91d3-09d12dddfbbd",
           "324222",
           "e1e1e1e1-e1e1-e1e1-e1e1-e1e1e1e1e1e1",
           "f47ac10b-58cc-4372-a567-0e02b2c3d479",
           "0"
          ],
          [
           "0f05a8f1-2bdf-4be7-8c82-4c9b58f04898",
           "193498",
           "e2e2e2e2-e2e2-e2e2-e2e2-e2e2e2e2e2e2",
           "f47ac10b-58cc-4372-a567-0e02b2c3d480",
           "19"
          ],
          [
           "1e2b47e6-499e-41c6-91d3-09d12dddfbbd",
           "345498",
           "e3e3e3e3-e3e3-e3e3-e3e3-e3e3e3e3e3e3",
           "f47ac10b-58cc-4372-a567-0e02b2c3d481",
           "21"
          ],
          [
           "1c4b0b50-1d5d-463a-b56e-1a6fd3aeb7d6",
           "245412",
           "e4e4e4e4-e4e4-e4e4-e4e4-e4e4e4e4e4e4",
           "f47ac10b-58cc-4372-a567-0e02b2c3d482",
           "34"
          ],
          [
           "34538e39-cd2e-4641-8d24-3c94146e6f16",
           "145467",
           "e5e5e5e5-e5e5-e5e5-e5e5-e5e5e5e5e5e5",
           "f47ac10b-58cc-4372-a567-0e02b2c3d483",
           "0"
          ],
          [
           "fa14c3ed-3c48-49f4-bd69-4d7f5b5f4b1b",
           "581530",
           "e6e6e6e6-e6e6-e6e6-e6e6-e6e6e6e6e6e6",
           "f47ac10b-58cc-4372-a567-0e02b2c3d484",
           "19"
          ],
          [
           "2e90f2b1-d237-47a6-96e8-6d01c0d78c3e",
           "45100",
           "e7e7e7e7-e7e7-e7e7-e7e7-e7e7e7e7e7e7",
           "f47ac10b-58cc-4372-a567-0e02b2c3d485",
           "19"
          ],
          [
           "acdb6f30-764f-404e-8b8e-7e7e3e6fa1a9",
           "565210",
           "e8e8e8e8-e8e8-e8e8-e8e8-e8e8e8e8e8e8",
           "f47ac10b-58cc-4372-a567-0e02b2c3d486",
           "21"
          ],
          [
           "5f0bdbdf-1d84-4c23-957c-8bb8c0ddc89d",
           "345310",
           "e9e9e9e9-e9e9-e9e9-e9e9-e9e9e9e9e9e9",
           "f47ac10b-58cc-4372-a567-0e02b2c3d487",
           "34"
          ],
          [
           "5f0bdbdf-1d84-4c23-957c-8bb8c0ddc89d",
           "345310",
           "ea9ea9ea-9ea9-9ea9-9ea9-9ea9ea9ea9ea",
           "f47ac10b-58cc-4372-a567-0e02b2c3d487",
           "34"
          ],
          [
           "27c59f76-5d26-4b82-a89b-59f8dfd2e9a7",
           "341315",
           "eb0eb0eb-0eb0-0eb0-0eb0-0eb0eb0eb0eb",
           "f47ac10b-58cc-4372-a567-0e02b2c3d488",
           "21"
          ],
          [
           "20dfef10-8f4e-45a1-82fc-123f4ab2a4a5",
           "291315",
           "ec1ec1ec-1ec1-1ec1-1ec1-1ec1ec1ec1ec",
           "f47ac10b-58cc-4372-a567-0e02b2c3d489",
           "0"
          ]
         ]
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "data",
         "type": "{\"containsNull\":true,\"elementType\":{\"fields\":[{\"metadata\":{},\"name\":\"companyId\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"grossValue\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"id\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"orderId\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"vat\",\"nullable\":true,\"type\":\"string\"}],\"type\":\"struct\"},\"type\":\"array\"}"
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "# 1. Define paths based on your Workspace structure\n",
    "base_path = \"/Workspace/Users/oriolds@icloud.com/IFCO Data Engineering Challenge/\"\n",
    "csv_path = os.path.join(base_path, \"orders.csv\") #\n",
    "json_path = os.path.join(base_path, \"invoicing_data.json\") #\n",
    "\n",
    "# 2. Read the files with Pandas to bypass driver restrictions\n",
    "print(\"1/4 Reading files with Pandas...\")\n",
    "pdf_orders = pd.read_csv(csv_path, sep=\";\") \n",
    "pdf_invoicing = pd.read_json(json_path) \n",
    "\n",
    "# 3. Convert to Spark DataFrames\n",
    "print(\"2/4 Converting to Spark...\")\n",
    "df_orders = spark.createDataFrame(pdf_orders)\n",
    "df_invoicing = spark.createDataFrame(pdf_invoicing)\n",
    "\n",
    "# 4. Sanitize column names for Delta Lake compatibility\n",
    "print(\"3/4 Sanitizing names...\")\n",
    "def sanitize_column_names(df):\n",
    "    for col_name in df.columns:\n",
    "        # Removes characters that Delta Lake doesn't like in column names\n",
    "        clean_name = re.sub(r'[\\s,;{}()\\n\\t=]', '_', col_name).strip('_')\n",
    "        df = df.withColumnRenamed(col_name, clean_name)\n",
    "    return df\n",
    "\n",
    "df_orders_bronze = sanitize_column_names(df_orders)\n",
    "df_invoicing_bronze = sanitize_column_names(df_invoicing)\n",
    "\n",
    "# 5. Save to Bronze tables with explicit schema overwriting\n",
    "print(\"4/4 Saving Bronze tables...\")\n",
    "df_orders_bronze.write.format(\"delta\").mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\").saveAsTable(\"default.bronze_orders\")\n",
    "\n",
    "df_invoicing_bronze.write.format(\"delta\").mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\").saveAsTable(\"default.bronze_invoicing\")\n",
    "\n",
    "print(\"Bronze Layer completed!\")\n",
    "\n",
    "# --- Visual verification ---\n",
    "print(\"\\nTable: default.bronze_orders\")\n",
    "display(spark.table(\"default.bronze_orders\").limit(5))\n",
    "\n",
    "print(\"\\nTable: default.bronze_invoicing\")\n",
    "display(spark.table(\"default.bronze_invoicing\").limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0542dd2a-e57b-43d0-a2f7-671309b4934e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Transformation Strategy: Moving from Bronze to Silver and Gold\n",
    "\n",
    "After successfully ingesting the raw data into the **Bronze Layer**, the pipeline follows a structured evolution to ensure data quality and business relevance. This approach separates technical data cleansing from high-level business logic.\n",
    "\n",
    "#### \uD83E\uDD48 Silver Layer: Technical Cleansing and Enrichment (Tests 2 & 3)\n",
    "The next stage focuses on transforming the raw data into a reliable, \"clean version of the truth\".\n",
    "* **Focus**: Data normalization, schema enforcement, and structural extraction.\n",
    "* **Implementation**: This layer addresses **Test 2 (Full Name Extraction)** and **Test 3 (Address Formatting)**.\n",
    "* **Justification**: These tasks involve parsing complex JSON strings and applying data integrity rules, such as using placeholders like \"John Doe\" or \"Unknown\". By resolving these at the Silver level, we ensure that all downstream processes use standardized contact information.\n",
    "\n",
    "#### \uD83E\uDD47 Gold Layer: Business Logic and Aggregations (Tests 1, 4 & 5)\n",
    "Once the data is cleaned and enriched, it moves to the **Gold Layer**, which is optimized for analytics and decision-making.\n",
    "* **Focus**: Complex business rules, financial calculations, and performance metrics.\n",
    "* **Implementation**: This layer executes **Test 1 (Crate Distribution)**, **Test 4 (Commission Calculations)**, and **Test 5 (Customer Catalog Consolidation)**.\n",
    "* **Justification**: These requirements involve multi-table joins, specific financial logic (calculating net values and percentage tiers), and entity resolution to handle duplicate companies. Separating these into Gold ensures that business KPIs remain flexible and independent of the underlying data cleaning logic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "143df3fb-553c-4b11-a280-8c953950446b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# \uD83E\uDD48 2. Silver Layer (Cleansed Data)\n",
    "**Objective:** Filter, cleanse and standardise the Bronze layer data to prepare it for joining and analysis. In this layer we enrich the tables and define the correct schemas (data types).\n",
    "\n",
    "## 2.1 Invoicing Data Standardisation (`invoicing_data`)\n",
    "**Actions performed:**\n",
    "* **Schema Evolution:** Unpacking (`explode`) of the nested JSON array to transform the hierarchy into relational rows and columns.\n",
    "* **Data Casting:** Conversion of financial fields from text (`string`) to precise numeric values (`double`) to enable future mathematical operations.\n",
    "* Implementation of idempotent logic to ensure the pipeline can be re-executed without errors.\n",
    "* Used raw SQL to avoid Refresh Table limitations in Databricks free tier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "249c8315-e27b-4bfb-a861-84832ca8cefd",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Json Flattening"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>companyId</th><th>grossValue</th><th>invoice_id</th><th>orderId</th><th>vat</th></tr></thead><tbody><tr><td>1e2b47e6-499e-41c6-91d3-09d12dddfbbd</td><td>324222.0</td><td>e1e1e1e1-e1e1-e1e1-e1e1-e1e1e1e1e1e1</td><td>f47ac10b-58cc-4372-a567-0e02b2c3d479</td><td>0.0</td></tr><tr><td>0f05a8f1-2bdf-4be7-8c82-4c9b58f04898</td><td>193498.0</td><td>e2e2e2e2-e2e2-e2e2-e2e2-e2e2e2e2e2e2</td><td>f47ac10b-58cc-4372-a567-0e02b2c3d480</td><td>19.0</td></tr><tr><td>1e2b47e6-499e-41c6-91d3-09d12dddfbbd</td><td>345498.0</td><td>e3e3e3e3-e3e3-e3e3-e3e3-e3e3e3e3e3e3</td><td>f47ac10b-58cc-4372-a567-0e02b2c3d481</td><td>21.0</td></tr><tr><td>1c4b0b50-1d5d-463a-b56e-1a6fd3aeb7d6</td><td>245412.0</td><td>e4e4e4e4-e4e4-e4e4-e4e4-e4e4e4e4e4e4</td><td>f47ac10b-58cc-4372-a567-0e02b2c3d482</td><td>34.0</td></tr><tr><td>34538e39-cd2e-4641-8d24-3c94146e6f16</td><td>145467.0</td><td>e5e5e5e5-e5e5-e5e5-e5e5-e5e5e5e5e5e5</td><td>f47ac10b-58cc-4372-a567-0e02b2c3d483</td><td>0.0</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "1e2b47e6-499e-41c6-91d3-09d12dddfbbd",
         324222.0,
         "e1e1e1e1-e1e1-e1e1-e1e1-e1e1e1e1e1e1",
         "f47ac10b-58cc-4372-a567-0e02b2c3d479",
         0.0
        ],
        [
         "0f05a8f1-2bdf-4be7-8c82-4c9b58f04898",
         193498.0,
         "e2e2e2e2-e2e2-e2e2-e2e2-e2e2e2e2e2e2",
         "f47ac10b-58cc-4372-a567-0e02b2c3d480",
         19.0
        ],
        [
         "1e2b47e6-499e-41c6-91d3-09d12dddfbbd",
         345498.0,
         "e3e3e3e3-e3e3-e3e3-e3e3-e3e3e3e3e3e3",
         "f47ac10b-58cc-4372-a567-0e02b2c3d481",
         21.0
        ],
        [
         "1c4b0b50-1d5d-463a-b56e-1a6fd3aeb7d6",
         245412.0,
         "e4e4e4e4-e4e4-e4e4-e4e4-e4e4e4e4e4e4",
         "f47ac10b-58cc-4372-a567-0e02b2c3d482",
         34.0
        ],
        [
         "34538e39-cd2e-4641-8d24-3c94146e6f16",
         145467.0,
         "e5e5e5e5-e5e5-e5e5-e5e5-e5e5e5e5e5e5",
         "f47ac10b-58cc-4372-a567-0e02b2c3d483",
         0.0
        ]
       ],
       "datasetInfos": [
        {
         "name": "_sqldf",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "companyId",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "grossValue",
            "nullable": true,
            "type": "double"
           },
           {
            "metadata": {},
            "name": "invoice_id",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "orderId",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "vat",
            "nullable": true,
            "type": "double"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.connect.dataframe.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 114
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "companyId",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "grossValue",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "invoice_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "orderId",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "vat",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "-- 1. Create the Silver table by flattening the 'data' array and casting types\n",
    "CREATE OR REPLACE TABLE default.silver_invoicing AS\n",
    "SELECT \n",
    "    invoice.companyId,\n",
    "    CAST(invoice.grossValue AS DOUBLE) AS grossValue,\n",
    "    invoice.id AS invoice_id,\n",
    "    invoice.orderId,\n",
    "    CAST(invoice.vat AS DOUBLE) AS vat\n",
    "FROM (\n",
    "    -- 'explode' turns the array into individual rows\n",
    "    SELECT explode(data) AS invoice \n",
    "    FROM default.bronze_invoicing\n",
    ");\n",
    "\n",
    "-- 2. Verify the schema and data\n",
    "DESCRIBE TABLE default.silver_invoicing;\n",
    "\n",
    "SELECT * FROM default.silver_invoicing LIMIT 5;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55b2ec6d-62fe-4e10-b645-e9559c27c010",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2.2 Orders Enrichment (`orders`)\n",
    "In this section we extract valuable information from nested JSON structures stored as plain text within the `contact_data` column, thereby fulfilling the format requirements for contacts and addresses.\n",
    "\n",
    "### Test 2: Contact Name Extraction\n",
    "**Objective:** Create a DataFrame (`df_1`) with `order_id` and `contact_full_name`.\n",
    "* The contact name and surname are dynamically extracted from the JSON using native Spark functions.\n",
    "* Null or empty values are handled by applying the placeholder `\"John Doe\"`.\n",
    "* **Validation:** A structured *Unit Test* is included to ensure the JSON parser works correctly before applying it to real data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca3f332b-672c-4fe8-a8eb-aff44637af78",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n\uD83E\uDDEA STARTING UNIT TEST: Name Extraction (Test 2)\n============================================================\nStep 1: Creating simulated data (Mock Data)...\n  Simulated scenario:\n  - Order 1: Has valid JSON with Name and Surname.\n  - Order 2: Has valid JSON but only Name.\n  - Order 3: Is 'null' (No contact data).\n\nStep 2: Processing data with 'get_contact_full_name'...\n\nStep 3: Running validations (Asserts)...\n  -> ✅ Order 1 correct: Extracted and concatenated 'Curtis Jackson'.\n  -> ✅ Order 2 correct: Handled the missing surname leaving only 'Maria'.\n  -> ✅ Order 3 correct: Applied the placeholder 'John Doe' upon detecting a null.\n\n\uD83C\uDFC6 RESULT: Unit Test passed! The JSON parser works perfectly.\n============================================================\n\n\uD83D\uDE80 Applying the extraction to the real orders table...\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>order_id</th><th>contact_full_name</th></tr></thead><tbody><tr><td>f47ac10b-58cc-4372-a567-0e02b2c3d479</td><td>Curtis Jackson</td></tr><tr><td>f47ac10b-58cc-4372-a567-0e02b2c3d480</td><td>Maria Theresa</td></tr><tr><td>f47ac10b-58cc-4372-a567-0e02b2c3d481</td><td>Para Cetamol</td></tr><tr><td>f47ac10b-58cc-4372-a567-0e02b2c3d482</td><td>John Doe</td></tr><tr><td>f47ac10b-58cc-4372-a567-0e02b2c3d483</td><td>John Doe</td></tr><tr><td>f47ac10b-58cc-4372-a567-0e02b2c3d484</td><td>John Krasinski</td></tr><tr><td>f47ac10b-58cc-4372-a567-0e02b2c3d485</td><td>John Doe</td></tr><tr><td>f47ac10b-58cc-4372-a567-0e02b2c3d486</td><td>Jennifer Lopez</td></tr><tr><td>f47ac10b-58cc-4372-a567-0e02b2c3d487</td><td>John Doe</td></tr><tr><td>f47ac10b-58cc-4372-a567-0e02b2c3d488</td><td>Curtis Jackson</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "f47ac10b-58cc-4372-a567-0e02b2c3d479",
         "Curtis Jackson"
        ],
        [
         "f47ac10b-58cc-4372-a567-0e02b2c3d480",
         "Maria Theresa"
        ],
        [
         "f47ac10b-58cc-4372-a567-0e02b2c3d481",
         "Para Cetamol"
        ],
        [
         "f47ac10b-58cc-4372-a567-0e02b2c3d482",
         "John Doe"
        ],
        [
         "f47ac10b-58cc-4372-a567-0e02b2c3d483",
         "John Doe"
        ],
        [
         "f47ac10b-58cc-4372-a567-0e02b2c3d484",
         "John Krasinski"
        ],
        [
         "f47ac10b-58cc-4372-a567-0e02b2c3d485",
         "John Doe"
        ],
        [
         "f47ac10b-58cc-4372-a567-0e02b2c3d486",
         "Jennifer Lopez"
        ],
        [
         "f47ac10b-58cc-4372-a567-0e02b2c3d487",
         "John Doe"
        ],
        [
         "f47ac10b-58cc-4372-a567-0e02b2c3d488",
         "Curtis Jackson"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "order_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "contact_full_name",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\uD83D\uDE80 Consolidating enriched data into the Silver Layer...\n✅ Table 'default.silver_orders' is now available in the catalog.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col, get_json_object, concat_ws, when, trim, lit\n",
    "\n",
    "# ==========================================\n",
    "# 1. FUNCTION DEFINITION (Silver Logic)\n",
    "# ==========================================\n",
    "def get_contact_full_name(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Extracts the first name and surname from the JSON in the 'contact_data' column.\n",
    "    If the field is null or cannot be extracted, returns 'John Doe'.\n",
    "    \"\"\"\n",
    "    # 1. Use get_json_object to navigate the JSON string.\n",
    "    # $[0] accesses the first array element, and .contact_name the key.\n",
    "    name = get_json_object(col(\"contact_data\"), \"$[0].contact_name\")\n",
    "    surname = get_json_object(col(\"contact_data\"), \"$[0].contact_surname\")\n",
    "    \n",
    "    # 2. Concatenate with a space in between and strip leading/trailing whitespace (trim)\n",
    "    full_name_raw = trim(concat_ws(\" \", name, surname))\n",
    "    \n",
    "    # 3. If the result is empty (because it was null), apply the placeholder 'John Doe'\n",
    "    final_full_name = when(\n",
    "        (full_name_raw == \"\") | full_name_raw.isNull(), \n",
    "        lit(\"John Doe\")\n",
    "    ).otherwise(full_name_raw)\n",
    "    \n",
    "    # 4. Return only the requested columns\n",
    "    return df.select(\n",
    "        col(\"order_id\"), \n",
    "        final_full_name.alias(\"contact_full_name\")\n",
    "    )\n",
    "\n",
    "# ==========================================\n",
    "# 2. VERBOSE UNIT TESTING (Demonstrating reliability)\n",
    "# ==========================================\n",
    "def test_get_contact_full_name():\n",
    "    print(\"=\"*60)\n",
    "    print(\"\uD83E\uDDEA STARTING UNIT TEST: Name Extraction (Test 2)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(\"Step 1: Creating simulated data (Mock Data)...\")\n",
    "    print(\"  Simulated scenario:\")\n",
    "    print(\"  - Order 1: Has valid JSON with Name and Surname.\")\n",
    "    print(\"  - Order 2: Has valid JSON but only Name.\")\n",
    "    print(\"  - Order 3: Is 'null' (No contact data).\")\n",
    "    \n",
    "    mock_data = [\n",
    "        (\"id-1\", '[{\"contact_name\":\"Curtis\", \"contact_surname\":\"Jackson\"}]'),\n",
    "        (\"id-2\", '[{\"contact_name\":\"Maria\"}]'),\n",
    "        (\"id-3\", None)\n",
    "    ]\n",
    "    mock_df = spark.createDataFrame(mock_data, [\"order_id\", \"contact_data\"])\n",
    "    \n",
    "    print(\"\\nStep 2: Processing data with 'get_contact_full_name'...\")\n",
    "    result_df = get_contact_full_name(mock_df)\n",
    "    results = result_df.collect()\n",
    "    \n",
    "    print(\"\\nStep 3: Running validations (Asserts)...\")\n",
    "    \n",
    "    # Validation 1: Name and Surname\n",
    "    val1 = [row.contact_full_name for row in results if row.order_id == \"id-1\"][0]\n",
    "    assert val1 == \"Curtis Jackson\", f\"Error in id-1. Expected: Curtis Jackson, Got: {val1}\"\n",
    "    print(\"  -> ✅ Order 1 correct: Extracted and concatenated 'Curtis Jackson'.\")\n",
    "    \n",
    "    # Validation 2: Name only (incomplete JSON)\n",
    "    val2 = [row.contact_full_name for row in results if row.order_id == \"id-2\"][0]\n",
    "    assert val2 == \"Maria\", f\"Error in id-2. Expected: Maria, Got: {val2}\"\n",
    "    print(\"  -> ✅ Order 2 correct: Handled the missing surname leaving only 'Maria'.\")\n",
    "    \n",
    "    # Validation 3: Placeholder (John Doe)\n",
    "    val3 = [row.contact_full_name for row in results if row.order_id == \"id-3\"][0]\n",
    "    assert val3 == \"John Doe\", f\"Error in id-3. Expected: John Doe, Got: {val3}\"\n",
    "    print(\"  -> ✅ Order 3 correct: Applied the placeholder 'John Doe' upon detecting a null.\")\n",
    "    \n",
    "    print(\"\\n\uD83C\uDFC6 RESULT: Unit Test passed! The JSON parser works perfectly.\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "# Run the test\n",
    "test_get_contact_full_name()\n",
    "\n",
    "# ==========================================\n",
    "# 3. APPLYING TO REAL DATA (Creating df_1)\n",
    "# ==========================================\n",
    "print(\"\uD83D\uDE80 Applying the extraction to the real orders table...\\n\")\n",
    "\n",
    "# Load the original orders table (located in the Bronze layer)\n",
    "df_orders = spark.table(\"default.bronze_orders\")\n",
    "\n",
    "# This is the df_1 requested by the exercise (Silver Layer)\n",
    "df_1 = get_contact_full_name(df_orders)\n",
    "\n",
    "# Visualise the final result\n",
    "display(df_1.limit(10))\n",
    "\n",
    "# ==========================================\n",
    "# PERSISTING THE SILVER LAYER\n",
    "# ==========================================\n",
    "print(\"\uD83D\uDE80 Consolidating enriched data into the Silver Layer...\")\n",
    "\n",
    "# Load the base bronze data\n",
    "df_bronze = spark.table(\"default.bronze_orders\")\n",
    "\n",
    "# Apply the Silver transformations defined in your functions\n",
    "df_silver_step1 = get_contact_full_name(df_bronze)\n",
    "df_silver_step2 = get_contact_address(df_bronze)\n",
    "\n",
    "# Join the enrichments back to create a complete Silver Orders table\n",
    "df_silver_orders = df_bronze.join(df_silver_step1, \"order_id\") \\\n",
    "                            .join(df_silver_step2, \"order_id\")\n",
    "\n",
    "# Save physically to the catalog\n",
    "df_silver_orders.write.format(\"delta\").mode(\"overwrite\") \\\n",
    "    .saveAsTable(\"default.silver_orders\")\n",
    "\n",
    "print(\"✅ Table 'default.silver_orders' is now available in the catalog.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8cbd663-7239-4235-87e9-f76f00b6208a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Test 3: Address Extraction and Formatting\n",
    "**Objective:** Create a DataFrame (`df_2`) with `order_id` and `contact_address`.\n",
    "* The city (`city`) and postal code (`cp`) fields are extracted from inside the JSON.\n",
    "* The format is standardised by combining them as `\"city, postal_code\"`.\n",
    "* Data quality rules are applied using specific placeholders (`\"Unknown\"` for city, `\"UNK00\"` for postal code) when the source information is null or missing.\n",
    "* **Validation:** The *Unit Test* exhaustively evaluates all possible permutations (both missing, one missing, or both present)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8908732f-a1b9-4d1e-a9ca-2818602c5b0d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n\uD83E\uDDEA STARTING UNIT TEST: Address Extraction (Test 3)\n============================================================\nStep 1: Creating simulated data (Mock Data)...\n  Cases to evaluate:\n  - Case A: Complete data (City and Postal Code)\n  - Case B: Missing Postal Code\n  - Case C: Missing City\n  - Case D: Both missing (Complete Null)\n\nStep 2: Processing data with 'get_contact_address'...\n\nStep 3: Running validations (Asserts)...\n  -> ✅ Case A: Correct formatting of both fields ('Chicago, 12345').\n  -> ✅ Case B: Correctly injected 'UNK00' when postal code was missing.\n  -> ✅ Case C: Correctly injected 'Unknown' when city was missing.\n  -> ✅ Case D: Handled the complete null returning 'Unknown, UNK00'.\n\n\uD83C\uDFC6 RESULT: Unit Test passed! The cleansing and formatting logic is correct.\n============================================================\n\n\uD83D\uDE80 Applying the extraction to the real orders table...\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>order_id</th><th>contact_address</th></tr></thead><tbody><tr><td>f47ac10b-58cc-4372-a567-0e02b2c3d479</td><td>Chicago, 12345</td></tr><tr><td>f47ac10b-58cc-4372-a567-0e02b2c3d480</td><td>Calcutta, UNK00</td></tr><tr><td>f47ac10b-58cc-4372-a567-0e02b2c3d481</td><td>Frankfurt am Oder, 3934</td></tr><tr><td>f47ac10b-58cc-4372-a567-0e02b2c3d482</td><td>Unknown, UNK00</td></tr><tr><td>f47ac10b-58cc-4372-a567-0e02b2c3d483</td><td>Unknown, UNK00</td></tr><tr><td>f47ac10b-58cc-4372-a567-0e02b2c3d484</td><td>New York, 1203</td></tr><tr><td>f47ac10b-58cc-4372-a567-0e02b2c3d485</td><td>Unknown, UNK00</td></tr><tr><td>f47ac10b-58cc-4372-a567-0e02b2c3d486</td><td>Esplugues de Llobregat, UNK00</td></tr><tr><td>f47ac10b-58cc-4372-a567-0e02b2c3d487</td><td>Unknown, UNK00</td></tr><tr><td>f47ac10b-58cc-4372-a567-0e02b2c3d488</td><td>Chicago, 12345</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "f47ac10b-58cc-4372-a567-0e02b2c3d479",
         "Chicago, 12345"
        ],
        [
         "f47ac10b-58cc-4372-a567-0e02b2c3d480",
         "Calcutta, UNK00"
        ],
        [
         "f47ac10b-58cc-4372-a567-0e02b2c3d481",
         "Frankfurt am Oder, 3934"
        ],
        [
         "f47ac10b-58cc-4372-a567-0e02b2c3d482",
         "Unknown, UNK00"
        ],
        [
         "f47ac10b-58cc-4372-a567-0e02b2c3d483",
         "Unknown, UNK00"
        ],
        [
         "f47ac10b-58cc-4372-a567-0e02b2c3d484",
         "New York, 1203"
        ],
        [
         "f47ac10b-58cc-4372-a567-0e02b2c3d485",
         "Unknown, UNK00"
        ],
        [
         "f47ac10b-58cc-4372-a567-0e02b2c3d486",
         "Esplugues de Llobregat, UNK00"
        ],
        [
         "f47ac10b-58cc-4372-a567-0e02b2c3d487",
         "Unknown, UNK00"
        ],
        [
         "f47ac10b-58cc-4372-a567-0e02b2c3d488",
         "Chicago, 12345"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "order_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "contact_address",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col, get_json_object, concat_ws, when, lit\n",
    "\n",
    "# ==========================================\n",
    "# 1. FUNCTION DEFINITION (Silver Logic)\n",
    "# ==========================================\n",
    "def get_contact_address(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Extracts the city and postal code from the JSON in 'contact_data'.\n",
    "    Applies placeholders 'Unknown' and 'UNK00' when data is missing.\n",
    "    Returns the format: 'city, postal_code'\n",
    "    \"\"\"\n",
    "    # 1. Extract city and postal code from the JSON\n",
    "    city_raw = get_json_object(col(\"contact_data\"), \"$[0].city\")\n",
    "    cp_raw = get_json_object(col(\"contact_data\"), \"$[0].cp\")\n",
    "    \n",
    "    # 2. Apply placeholder logic (when null or empty string)\n",
    "    city_clean = when(city_raw.isNull() | (city_raw == \"\"), lit(\"Unknown\")).otherwise(city_raw)\n",
    "    cp_clean = when(cp_raw.isNull() | (cp_raw == \"\"), lit(\"UNK00\")).otherwise(cp_raw)\n",
    "    \n",
    "    # 3. Concatenate with comma and space\n",
    "    formatted_address = concat_ws(\", \", city_clean, cp_clean)\n",
    "    \n",
    "    # 4. Return only the required columns\n",
    "    return df.select(\n",
    "        col(\"order_id\"), \n",
    "        formatted_address.alias(\"contact_address\")\n",
    "    )\n",
    "\n",
    "# ==========================================\n",
    "# 2. VERBOSE UNIT TESTING\n",
    "# ==========================================\n",
    "def test_get_contact_address():\n",
    "    print(\"=\"*60)\n",
    "    print(\"\uD83E\uDDEA STARTING UNIT TEST: Address Extraction (Test 3)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(\"Step 1: Creating simulated data (Mock Data)...\")\n",
    "    print(\"  Cases to evaluate:\")\n",
    "    print(\"  - Case A: Complete data (City and Postal Code)\")\n",
    "    print(\"  - Case B: Missing Postal Code\")\n",
    "    print(\"  - Case C: Missing City\")\n",
    "    print(\"  - Case D: Both missing (Complete Null)\")\n",
    "    \n",
    "    mock_data = [\n",
    "        (\"id-A\", '[{\"city\":\"Chicago\", \"cp\":\"12345\"}]'),\n",
    "        (\"id-B\", '[{\"city\":\"Calcutta\"}]'),\n",
    "        (\"id-C\", '[{\"cp\":\"3934\"}]'),\n",
    "        (\"id-D\", None)\n",
    "    ]\n",
    "    mock_df = spark.createDataFrame(mock_data, [\"order_id\", \"contact_data\"])\n",
    "    \n",
    "    print(\"\\nStep 2: Processing data with 'get_contact_address'...\")\n",
    "    result_df = get_contact_address(mock_df)\n",
    "    results = result_df.collect()\n",
    "    \n",
    "    print(\"\\nStep 3: Running validations (Asserts)...\")\n",
    "    \n",
    "    # Validation A: Both fields present\n",
    "    valA = [row.contact_address for row in results if row.order_id == \"id-A\"][0]\n",
    "    assert valA == \"Chicago, 12345\", f\"Error in id-A. Got: {valA}\"\n",
    "    print(\"  -> ✅ Case A: Correct formatting of both fields ('Chicago, 12345').\")\n",
    "    \n",
    "    # Validation B: Missing postal code\n",
    "    valB = [row.contact_address for row in results if row.order_id == \"id-B\"][0]\n",
    "    assert valB == \"Calcutta, UNK00\", f\"Error in id-B. Got: {valB}\"\n",
    "    print(\"  -> ✅ Case B: Correctly injected 'UNK00' when postal code was missing.\")\n",
    "    \n",
    "    # Validation C: Missing city\n",
    "    valC = [row.contact_address for row in results if row.order_id == \"id-C\"][0]\n",
    "    assert valC == \"Unknown, 3934\", f\"Error in id-C. Got: {valC}\"\n",
    "    print(\"  -> ✅ Case C: Correctly injected 'Unknown' when city was missing.\")\n",
    "    \n",
    "    # Validation D: Everything null\n",
    "    valD = [row.contact_address for row in results if row.order_id == \"id-D\"][0]\n",
    "    assert valD == \"Unknown, UNK00\", f\"Error in id-D. Got: {valD}\"\n",
    "    print(\"  -> ✅ Case D: Handled the complete null returning 'Unknown, UNK00'.\")\n",
    "    \n",
    "    print(\"\\n\uD83C\uDFC6 RESULT: Unit Test passed! The cleansing and formatting logic is correct.\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "# Run the test\n",
    "test_get_contact_address()\n",
    "\n",
    "# ==========================================\n",
    "# 3. APPLYING TO REAL DATA (Creating df_2)\n",
    "# ==========================================\n",
    "print(\"\uD83D\uDE80 Applying the extraction to the real orders table...\\n\")\n",
    "\n",
    "df_orders = spark.table(\"default.bronze_orders\")\n",
    "\n",
    "# This is the df_2 requested by the exercise (Silver Layer)\n",
    "df_2 = get_contact_address(df_orders)\n",
    "\n",
    "# Visualise the result\n",
    "display(df_2.limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8cb7b514-183b-4a6a-ba0a-9130c5cb60b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# \uD83E\uDD47 3. Gold Layer (Business Aggregations)\n",
    "**Objective:** Consume the cleaned and enriched data from the Silver layer to generate summary tables, business metrics, and KPIs. These tables are designed for direct consumption by business teams, BI tools (Power BI, Tableau), or Machine Learning models.\n",
    "\n",
    "In this layer, we address specific business analytical requirements:\n",
    "* **Test 1:** Distribution of box types per company.\n",
    "* **Test 4:** Financial calculation of commissions for the sales team.\n",
    "* **Test 5:** Consolidation of the customer catalog and their assigned sales representatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5606316f-e100-4274-835b-bf4d90bcc5df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3.1 Test 1: Crate Type Distribution per Company (Relocated)\n",
    "\n",
    "> **Architectural Assumption:** This section fulfills the requirement for **Test 1**. Although it appears first in the challenge, it is implemented here in the **Gold Layer** to maintain a professional Medallion Architecture. Calculating business distributions at this stage ensures the analysis is performed on deduplicated and validated data.\n",
    "\n",
    "The goal of this analysis is to determine the operational footprint of each client by calculating the volume and variety of equipment used. Specifically, we are:\n",
    "* **Aggregating** total orders by grouping unique company names with their respective crate categories (e.g., Plastic, Wood, Metal).\n",
    "* **Quantifying** the distribution to identify which equipment types are most prevalent for each business partner.\n",
    "* **Refining** the output to provide a clean, sorted dataset ready for the final Executive Dashboard (Test 6)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8824aba1-fa5f-47e2-8721-66f81aad952c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n\uD83E\uDDEA STARTING UNIT TEST: Crate Distribution (Test 1)\n============================================================\nStep 1: Creating mock data...\n  Scenario details:\n  - Acme Corp: 2 Wood crates, 1 Plastic crate.\n  - Globex: 1 Metal crate.\n\nStep 2: Processing data through 'calculate_crate_distribution'...\n\nStep 3: Running assertions...\n  -> ✅ Acme Corp - Wood: Count validated (2).\n  -> ✅ Acme Corp - Plastic: Count validated (1).\n  -> ✅ Globex - Metal: Count validated (1).\n\n\uD83C\uDFC6 RESULT: Unit Test passed! Distribution logic is correct.\n============================================================\n\n\uD83D\uDE80 Running the crate distribution pipeline on Silver data...\n\nTest 1: Crate distribution calculated successfully from Silver data.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col, desc\n",
    "\n",
    "# ==========================================\n",
    "# 1. FUNCTION DEFINITION (Gold/Silver Logic)\n",
    "# ==========================================\n",
    "def calculate_crate_distribution(df_orders_silver: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Groups orders by company and crate type to calculate the total distribution.\n",
    "    Returns a sorted DataFrame with the count per combination.\n",
    "    \"\"\"\n",
    "    return (df_orders_silver\n",
    "        .groupBy(\"company_name\", \"crate_type\")\n",
    "        .count()\n",
    "        .withColumnRenamed(\"count\", \"total_crates\")\n",
    "        .orderBy(\"company_name\", desc(\"total_crates\"))\n",
    "    )\n",
    "\n",
    "# ==========================================\n",
    "# 2. VERBOSE UNIT TESTING\n",
    "# ==========================================\n",
    "def test_calculate_crate_distribution():\n",
    "    print(\"=\"*60)\n",
    "    print(\"\uD83E\uDDEA STARTING UNIT TEST: Crate Distribution (Test 1)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(\"Step 1: Creating mock data...\")\n",
    "    print(\"  Scenario details:\")\n",
    "    print(\"  - Acme Corp: 2 Wood crates, 1 Plastic crate.\")\n",
    "    print(\"  - Globex: 1 Metal crate.\")\n",
    "    \n",
    "    mock_data = [\n",
    "        (\"Acme Corp\", \"Wood\"),\n",
    "        (\"Acme Corp\", \"Wood\"),\n",
    "        (\"Acme Corp\", \"Plastic\"),\n",
    "        (\"Globex\", \"Metal\")\n",
    "    ]\n",
    "    mock_schema = [\"company_name\", \"crate_type\"]\n",
    "    mock_df = spark.createDataFrame(mock_data, mock_schema)\n",
    "    \n",
    "    print(\"\\nStep 2: Processing data through 'calculate_crate_distribution'...\")\n",
    "    result_df = calculate_crate_distribution(mock_df)\n",
    "    \n",
    "    # Convert result to a dictionary for easier assertion: {(company, type): count}\n",
    "    results = { (row.company_name, row.crate_type): row.total_crates for row in result_df.collect() }\n",
    "    \n",
    "    print(\"\\nStep 3: Running assertions...\")\n",
    "    \n",
    "    # Check Acme Corp Wood\n",
    "    acme_wood = results.get((\"Acme Corp\", \"Wood\"))\n",
    "    assert acme_wood == 2, f\"Error for Acme Wood. Expected 2, got: {acme_wood}\"\n",
    "    print(f\"  -> ✅ Acme Corp - Wood: Count validated ({acme_wood}).\")\n",
    "    \n",
    "    # Check Acme Corp Plastic\n",
    "    acme_plastic = results.get((\"Acme Corp\", \"Plastic\"))\n",
    "    assert acme_plastic == 1, f\"Error for Acme Plastic. Expected 1, got: {acme_plastic}\"\n",
    "    print(f\"  -> ✅ Acme Corp - Plastic: Count validated ({acme_plastic}).\")\n",
    "    \n",
    "    # Check Globex Metal\n",
    "    globex_metal = results.get((\"Globex\", \"Metal\"))\n",
    "    assert globex_metal == 1, f\"Error for Globex Metal. Expected 1, got: {globex_metal}\"\n",
    "    print(f\"  -> ✅ Globex - Metal: Count validated ({globex_metal}).\")\n",
    "    \n",
    "    print(\"\\n\uD83C\uDFC6 RESULT: Unit Test passed! Distribution logic is correct.\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "# Execute the test\n",
    "test_calculate_crate_distribution()\n",
    "\n",
    "# ==========================================\n",
    "# 3. PRODUCTION EXECUTION\n",
    "# ==========================================\n",
    "print(\"\uD83D\uDE80 Running the crate distribution pipeline on Silver data...\\n\")\n",
    "\n",
    "# Load the cleansed data from the Silver layer\n",
    "df_orders_silver = spark.table(\"default.silver_orders\")\n",
    "\n",
    "# Apply the logic\n",
    "df_crate_distribution = calculate_crate_distribution(df_orders_silver)\n",
    "\n",
    "print(\"Test 1: Crate distribution calculated successfully from Silver data.\")\n",
    "\n",
    "# Create the Gold table for Test 1\n",
    "df_gold_distribution = calculate_crate_distribution(spark.table(\"default.silver_orders\"))\n",
    "\n",
    "df_gold_distribution.write.mode(\"overwrite\").format(\"delta\") \\\n",
    "    .saveAsTable(\"default.gold_crate_distribution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5741671b-7b73-46da-b48f-1f16a607792f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## 3.2 Test 4: Sales Team Commission Calculation\n",
    "**Objective:** Calculate the commissions in euros for each sales representative based on their level of participation in the sale.\n",
    "* **Cross-Layer JOIN:** The `orders` table is joined with `invoicing_data` using the `order_id`.\n",
    "* **Financial Calculations:** The net value is calculated by deducting VAT (`vat`) from the gross value (`grossValue`) and converting cents to euros.\n",
    "* **Hierarchy Handling:** The `posexplode` function is used to unpack the list of sales representatives while preserving their index (position) to apply the correct percentages (6%, 2.5% and 0.95%).\n",
    "* **Validation:** The *Unit Test* joins simulated data and verifies that the arithmetic and rounding are correct (down to the cent).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dac65db0-245e-4ead-8ff0-627590989518",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33b2ba69-ef8b-4913-8331-e2230c73c2c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n\uD83E\uDDEA STARTING UNIT TEST: Sales Commissions (Test 4)\n============================================================\nStep 1: Creating mock data...\n  Scenario details:\n  - Order 'ORD-1' with Gross: 11,900 cents and 19% VAT.\n    -> Calculated Net: 10,000 cents (100.00 Euros).\n  - Salesowners: 'Luke, Leia, Han, Chewbacca'.\n    -> Luke (Rank 0) should earn 6.00 € (6%)\n    -> Leia (Rank 1) should earn 2.50 € (2.5%)\n    -> Han (Rank 2) should earn 0.95 € (0.95%)\n    -> Chewbacca (Rank 3) should earn 0.00 € (0%)\n\nStep 2: Processing data through 'calculate_sales_commissions'...\n\nStep 3: Running mathematical assertions...\n  -> ✅ Main Owner (Luke): Exact commission validated (6.00 €).\n  -> ✅ Co-owner 1 (Leia): Exact commission validated (2.50 €).\n  -> ✅ Co-owner 2 (Han): Exact commission validated (0.95 €).\n  -> ✅ Others (Chewbacca): Correctly excluded from commission (0.00 €).\n\n\uD83C\uDFC6 RESULT: Unit Test passed! Financial logic is 100% correct.\n============================================================\n\n\uD83D\uDE80 Running the commission pipeline on Silver tables...\n\n\uD83C\uDFC6 Sales commission report generated successfully.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>salesowner_name</th><th>commission_euros</th></tr></thead><tbody><tr><td>Leonard Cohen</td><td>650.31</td></tr><tr><td>David Henderson</td><td>487.77</td></tr><tr><td>Luke Skywalker</td><td>377.61</td></tr><tr><td>Yuri Gagarin</td><td>309.23</td></tr><tr><td>David Goliat</td><td>279.38</td></tr><tr><td>Ammy Winehouse</td><td>209.52</td></tr><tr><td>Marianov Merschik</td><td>188.61</td></tr><tr><td>Chris Pratt</td><td>114.08</td></tr><tr><td>Vladimir Chukov</td><td>72.83</td></tr><tr><td>Marie Curie</td><td>70.52</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Leonard Cohen",
         650.31
        ],
        [
         "David Henderson",
         487.77
        ],
        [
         "Luke Skywalker",
         377.61
        ],
        [
         "Yuri Gagarin",
         309.23
        ],
        [
         "David Goliat",
         279.38
        ],
        [
         "Ammy Winehouse",
         209.52
        ],
        [
         "Marianov Merschik",
         188.61
        ],
        [
         "Chris Pratt",
         114.08
        ],
        [
         "Vladimir Chukov",
         72.83
        ],
        [
         "Marie Curie",
         70.52
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "salesowner_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "commission_euros",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col, split, posexplode, sum, round, when, trim\n",
    "\n",
    "# ==========================================\n",
    "# 1. FUNCTION DEFINITION (Gold Logic)\n",
    "# ==========================================\n",
    "def calculate_sales_commissions(df_orders: DataFrame, df_invoicing: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Joins orders and invoicing data, calculates net value, and distributes commissions \n",
    "    based on the salesowner's position in the list.\n",
    "    \"\"\"\n",
    "    # 1. Join between orders and invoicing_data using order_id\n",
    "    df_joined = df_orders.join(\n",
    "        df_invoicing, \n",
    "        df_orders.order_id == df_invoicing.orderId, \n",
    "        \"inner\"\n",
    "    )\n",
    "    \n",
    "    # 2. Calculate net value in cents: Net = Gross / (1 + VAT/100)\n",
    "    df_net = df_joined.withColumn(\n",
    "        \"net_value_cents\", \n",
    "        col(\"grossValue\") / (1 + (col(\"vat\") / 100))\n",
    "    )\n",
    "    \n",
    "    # 3. Explode salesowners while preserving their position (index)\n",
    "    # We split by ',' and use trim() to remove any leading/trailing whitespace\n",
    "    df_exploded = df_net.select(\n",
    "        \"order_id\",\n",
    "        \"net_value_cents\",\n",
    "        posexplode(split(col(\"salesowners\"), \",\")).alias(\"position\", \"salesowner_name\")\n",
    "    )\n",
    "    \n",
    "    # Clean up whitespace for accurate grouping\n",
    "    df_exploded = df_exploded.withColumn(\"salesowner_name\", trim(col(\"salesowner_name\")))\n",
    "    \n",
    "    # 4. Assign commission percentages according to rank (position)\n",
    "    df_commissions = df_exploded.withColumn(\"commission_cents\",\n",
    "        when(col(\"position\") == 0, col(\"net_value_cents\") * 0.06)      # Main Owner: 6%\n",
    "        .when(col(\"position\") == 1, col(\"net_value_cents\") * 0.025)    # Co-owner 1: 2.5%\n",
    "        .when(col(\"position\") == 2, col(\"net_value_cents\") * 0.0095)   # Co-owner 2: 0.95%\n",
    "        .otherwise(0.0)                                                # Others: 0%\n",
    "    )\n",
    "    \n",
    "    # 5. Group by salesowner, sum, convert cents to euros (/100), and round\n",
    "    df_final = (df_commissions\n",
    "        .groupBy(\"salesowner_name\")\n",
    "        .agg(sum(\"commission_cents\").alias(\"total_commission_cents\"))\n",
    "        .withColumn(\"commission_euros\", round(col(\"total_commission_cents\") / 100, 2))\n",
    "        .select(\"salesowner_name\", \"commission_euros\")\n",
    "        .orderBy(col(\"commission_euros\").desc()) # Sort by highest earners first\n",
    "    )\n",
    "    \n",
    "    return df_final\n",
    "\n",
    "# ==========================================\n",
    "# 2. VERBOSE UNIT TESTING\n",
    "# ==========================================\n",
    "def test_calculate_sales_commissions():\n",
    "    print(\"=\"*60)\n",
    "    print(\"\uD83E\uDDEA STARTING UNIT TEST: Sales Commissions (Test 4)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(\"Step 1: Creating mock data...\")\n",
    "    print(\"  Scenario details:\")\n",
    "    print(\"  - Order 'ORD-1' with Gross: 11,900 cents and 19% VAT.\")\n",
    "    print(\"    -> Calculated Net: 10,000 cents (100.00 Euros).\")\n",
    "    print(\"  - Salesowners: 'Luke, Leia, Han, Chewbacca'.\")\n",
    "    print(\"    -> Luke (Rank 0) should earn 6.00 € (6%)\")\n",
    "    print(\"    -> Leia (Rank 1) should earn 2.50 € (2.5%)\")\n",
    "    print(\"    -> Han (Rank 2) should earn 0.95 € (0.95%)\")\n",
    "    print(\"    -> Chewbacca (Rank 3) should earn 0.00 € (0%)\")\n",
    "    \n",
    "    mock_orders = spark.createDataFrame(\n",
    "        [(\"ORD-1\", \"Luke, Leia, Han, Chewbacca\")], \n",
    "        [\"order_id\", \"salesowners\"]\n",
    "    )\n",
    "    mock_invoicing = spark.createDataFrame(\n",
    "        [(\"ORD-1\", 11900.0, 19.0)], \n",
    "        [\"orderId\", \"grossValue\", \"vat\"]\n",
    "    )\n",
    "    \n",
    "    print(\"\\nStep 2: Processing data through 'calculate_sales_commissions'...\")\n",
    "    result_df = calculate_sales_commissions(mock_orders, mock_invoicing)\n",
    "    results = {row.salesowner_name: row.commission_euros for row in result_df.collect()}\n",
    "    \n",
    "    print(\"\\nStep 3: Running mathematical assertions...\")\n",
    "    \n",
    "    assert results.get(\"Luke\") == 6.00, f\"Error for Luke. Expected 6.00, got: {results.get('Luke')}\"\n",
    "    print(\"  -> ✅ Main Owner (Luke): Exact commission validated (6.00 €).\")\n",
    "    \n",
    "    assert results.get(\"Leia\") == 2.50, f\"Error for Leia. Expected 2.50, got: {results.get('Leia')}\"\n",
    "    print(\"  -> ✅ Co-owner 1 (Leia): Exact commission validated (2.50 €).\")\n",
    "    \n",
    "    assert results.get(\"Han\") == 0.95, f\"Error for Han. Expected 0.95, got: {results.get('Han')}\"\n",
    "    print(\"  -> ✅ Co-owner 2 (Han): Exact commission validated (0.95 €).\")\n",
    "    \n",
    "    assert results.get(\"Chewbacca\") == 0.00, f\"Error for Chewbacca. Expected 0.00, got: {results.get('Chewbacca')}\"\n",
    "    print(\"  -> ✅ Others (Chewbacca): Correctly excluded from commission (0.00 €).\")\n",
    "    \n",
    "    print(\"\\n\uD83C\uDFC6 RESULT: Unit Test passed! Financial logic is 100% correct.\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "# Execute the test\n",
    "test_calculate_sales_commissions()\n",
    "\n",
    "# ==========================================\n",
    "# 3. PRODUCTION EXECUTION\n",
    "# ==========================================\n",
    "print(\"\uD83D\uDE80 Running the commission pipeline on Silver tables...\\n\")\n",
    "\n",
    "\n",
    "df_orders = spark.table(\"default.silver_orders\")\n",
    "df_invoicing = spark.table(\"default.silver_invoicing\")\n",
    "\n",
    "# The function logic remains the same\n",
    "df_commission_report = calculate_sales_commissions(df_orders, df_invoicing)\n",
    "\n",
    "# Save the final result as a Gold table to keep the catalog clean\n",
    "df_commission_report.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"default.gold_sales_commissions\")\n",
    "\n",
    "print(\"\uD83C\uDFC6 Sales commission report generated successfully.\")\n",
    "display(spark.table(\"default.gold_sales_commissions\").limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6947c219-180f-476a-9c79-a75dcf0bb85e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3.3 Test 5: Customer and Assigned Sales Representatives Consolidation\n",
    "**Objective:** Generate a DataFrame (`df_3`) with a unique company catalogue and a sorted list of all sales representatives who have worked with each company.\n",
    "* **Duplicate Consolidation (Data Quality):** Addressing the requirement regarding duplicate customers, a company name normalisation is implemented (removing special characters, spaces and converting to lowercase) to use it as a grouping key. This merges identical entities that have multiple IDs.\n",
    "* **Array Handling:** `explode` is used to separate sales representatives, `collect_set` to obtain an array of unique elements per company, and `array_sort` to guarantee alphabetical order.\n",
    "* **Validation:** The *Unit Test* simulates a duplicate company under two different IDs, validates the deduplication of repeated sales representatives and checks the strict alphabetical order of the final list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1fb045dd-7eb0-4ada-babb-dc3186aa9f22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n\uD83E\uDDEA STARTING UNIT TEST: Company Catalogue (Test 5)\n============================================================\nStep 1: Creating simulated data (Mock Data)...\n  Simulated scenario:\n  - Record 1: 'Veggie Inc' (ID-1) served by 'Zack, Alice'\n  - Record 2: 'veggie inc.' (ID-2) served by 'Bob, Alice' -> IT IS A DUPLICATE!\n    -> Expected Result: A single consolidated row.\n    -> Expected Sales Reps: 'Alice, Bob, Zack' (Unique and in alphabetical order).\n\nStep 2: Processing data with 'get_companies_with_salesowners'...\n\nStep 3: Running validations (Asserts)...\n  -> ✅ Duplicate Consolidation: Name variations and IDs were merged into 1 row.\n  -> ✅ Sales Rep Handling: The list removed duplicates and sorted perfectly (Alice, Bob, Zack).\n\n\uD83C\uDFC6 RESULT: Unit Test passed! The cleansing and consolidation logic is top-notch.\n============================================================\n\n\uD83D\uDE80 Running the consolidation on the real database...\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>company_id</th><th>company_name</th><th>list_salesowners</th></tr></thead><tbody><tr><td>5c17d142-4b21-4293-8a34-8dcd2bc24f82</td><td>Fresh Farms Ltd</td><td>Chris Pratt, David Goliat, Marie Curie</td></tr><tr><td>20dfef10-8f4e-45a1-82fc-123f4ab2a4a5</td><td>healthy snacks c.o.</td><td>Luke Skywalker, Marianov Merschik, Vladimir Chukov</td></tr><tr><td>7d4b212e-29e5-4f2a-9b28-745a3c7f0b60</td><td>Organic Veggies Ltd</td><td>Ammy Winehouse, David Henderson, Leonard Cohen</td></tr><tr><td>8f1c5d4a-9045-4be5-bb38-7f587f478a92</td><td>Farm Fresh Co</td><td>Ammy Winehouse, Chris Pratt, Leonard Cohen</td></tr><tr><td>f712dc3d-4681-4ec6-9b76-bf47b4ccf5b2</td><td>Green Organic Co</td><td>Chris Pratt, Leon Leonov</td></tr><tr><td>4a7561b1-1de1-420a-93ed-2c12a5bbd1ab</td><td>Farms Global Co</td><td>David Goliat, Leonard Cohen</td></tr><tr><td>9b31b19f-69a2-4aeb-8f6e-f4b8d2f9c12a</td><td>Veggies Unlimited</td><td>Ammy Winehouse, Leon Leonov</td></tr><tr><td>34538e39-cd2e-4641-8d24-3c94146e6f16</td><td>Meat Packers Ltd</td><td>Chris Pratt, David Henderson, Leon Leonov, Marianov Merschik</td></tr><tr><td>83df789a-b30c-4a1b-8e67-1f512bfa20c7</td><td>Tropical Fresh Co</td><td>Ammy Winehouse, David Henderson, Leonard Cohen</td></tr><tr><td>0d09ae2b-d9a5-4d67-bb97-963be9379b4e</td><td>Healthy Eats Ltd</td><td>Ammy Winehouse, Yuri Gagarin</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "5c17d142-4b21-4293-8a34-8dcd2bc24f82",
         "Fresh Farms Ltd",
         "Chris Pratt, David Goliat, Marie Curie"
        ],
        [
         "20dfef10-8f4e-45a1-82fc-123f4ab2a4a5",
         "healthy snacks c.o.",
         "Luke Skywalker, Marianov Merschik, Vladimir Chukov"
        ],
        [
         "7d4b212e-29e5-4f2a-9b28-745a3c7f0b60",
         "Organic Veggies Ltd",
         "Ammy Winehouse, David Henderson, Leonard Cohen"
        ],
        [
         "8f1c5d4a-9045-4be5-bb38-7f587f478a92",
         "Farm Fresh Co",
         "Ammy Winehouse, Chris Pratt, Leonard Cohen"
        ],
        [
         "f712dc3d-4681-4ec6-9b76-bf47b4ccf5b2",
         "Green Organic Co",
         "Chris Pratt, Leon Leonov"
        ],
        [
         "4a7561b1-1de1-420a-93ed-2c12a5bbd1ab",
         "Farms Global Co",
         "David Goliat, Leonard Cohen"
        ],
        [
         "9b31b19f-69a2-4aeb-8f6e-f4b8d2f9c12a",
         "Veggies Unlimited",
         "Ammy Winehouse, Leon Leonov"
        ],
        [
         "34538e39-cd2e-4641-8d24-3c94146e6f16",
         "Meat Packers Ltd",
         "Chris Pratt, David Henderson, Leon Leonov, Marianov Merschik"
        ],
        [
         "83df789a-b30c-4a1b-8e67-1f512bfa20c7",
         "Tropical Fresh Co",
         "Ammy Winehouse, David Henderson, Leonard Cohen"
        ],
        [
         "0d09ae2b-d9a5-4d67-bb97-963be9379b4e",
         "Healthy Eats Ltd",
         "Ammy Winehouse, Yuri Gagarin"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "company_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "company_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "list_salesowners",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\uD83D\uDE80 Running the consolidation and saving to Gold Layer...\n\n\uD83C\uDFC6 Table 'default.gold_companies_salesowners' created successfully.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col, split, explode, trim, collect_set, array_sort, array_join, first, lower, regexp_replace\n",
    "\n",
    "# ==========================================\n",
    "# 1. FUNCTION DEFINITION (Gold Logic)\n",
    "# ==========================================\n",
    "def get_companies_with_salesowners(df_orders: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Consolidates companies by resolving duplicates and generates a unique, \n",
    "    comma-separated list of their salesowners sorted alphabetically.\n",
    "    \"\"\"\n",
    "    # 1. Sales Representative Cleaning (Nested Generator fix):\n",
    "    # First we explode, then in a separate step we trim\n",
    "    df_exploded = (df_orders\n",
    "        .withColumn(\"salesowner_raw\", explode(split(col(\"salesowners\"), \",\")))\n",
    "        .withColumn(\"salesowner\", trim(col(\"salesowner_raw\")))\n",
    "    )\n",
    "    \n",
    "    # 2. Company Name Normalization (the master key against duplicates)\n",
    "    df_norm = df_exploded.withColumn(\n",
    "        \"normalized_name\", \n",
    "        regexp_replace(lower(col(\"company_name\")), \"[^a-z0-9]\", \"\")\n",
    "    )\n",
    "    \n",
    "    # 3. Grouping and Consolidation\n",
    "    df_grouped = df_norm.groupBy(\"normalized_name\").agg(\n",
    "        first(\"company_id\").alias(\"company_id\"),      # Keep any representative company_id\n",
    "        first(\"company_name\").alias(\"company_name\"),  # Keep any representative company_name\n",
    "        array_sort(collect_set(\"salesowner\")).alias(\"unique_salesowners\") \n",
    "    )\n",
    "    \n",
    "    # 4. Format the resulting array as a comma-separated String\n",
    "    df_final = df_grouped.withColumn(\n",
    "        \"list_salesowners\", \n",
    "        array_join(col(\"unique_salesowners\"), \", \")\n",
    "    ).select(\"company_id\", \"company_name\", \"list_salesowners\")\n",
    "    \n",
    "    return df_final\n",
    "\n",
    "# ==========================================\n",
    "# 2. VERBOSE UNIT TESTING\n",
    "# ==========================================\n",
    "def test_get_companies_with_salesowners():\n",
    "    print(\"=\"*60)\n",
    "    print(\"\uD83E\uDDEA STARTING UNIT TEST: Company Catalogue (Test 5)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(\"Step 1: Creating simulated data (Mock Data)...\")\n",
    "    print(\"  Simulated scenario:\")\n",
    "    print(\"  - Record 1: 'Veggie Inc' (ID-1) served by 'Zack, Alice'\")\n",
    "    print(\"  - Record 2: 'veggie inc.' (ID-2) served by 'Bob, Alice' -> IT IS A DUPLICATE!\")\n",
    "    print(\"    -> Expected Result: A single consolidated row.\")\n",
    "    print(\"    -> Expected Sales Reps: 'Alice, Bob, Zack' (Unique and in alphabetical order).\")\n",
    "    \n",
    "    mock_data = [\n",
    "        (\"ID-1\", \"Veggie Inc\", \"Zack, Alice\"),\n",
    "        (\"ID-2\", \"veggie inc.\", \"Bob, Alice\")\n",
    "    ]\n",
    "    mock_df = spark.createDataFrame(mock_data, [\"company_id\", \"company_name\", \"salesowners\"])\n",
    "    \n",
    "    print(\"\\nStep 2: Processing data with 'get_companies_with_salesowners'...\")\n",
    "    result_df = get_companies_with_salesowners(mock_df)\n",
    "    results = result_df.collect()\n",
    "    \n",
    "    print(\"\\nStep 3: Running validations (Asserts)...\")\n",
    "    \n",
    "    # Validation 1: Duplicate consolidation\n",
    "    num_companies = len(results)\n",
    "    assert num_companies == 1, f\"Error: Expected 1 consolidated company, got {num_companies}\"\n",
    "    print(\"  -> ✅ Duplicate Consolidation: Name variations and IDs were merged into 1 row.\")\n",
    "    \n",
    "    # Validation 2: Deduplication and Alphabetical Order\n",
    "    final_list = results[0].list_salesowners\n",
    "    assert final_list == \"Alice, Bob, Zack\", f\"Error in the list. Got: {final_list}\"\n",
    "    print(\"  -> ✅ Sales Rep Handling: The list removed duplicates and sorted perfectly (Alice, Bob, Zack).\")\n",
    "    \n",
    "    print(\"\\n\uD83C\uDFC6 RESULT: Unit Test passed! The cleansing and consolidation logic is top-notch.\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "# Run the test\n",
    "test_get_companies_with_salesowners()\n",
    "\n",
    "# ==========================================\n",
    "# 3. APPLICATION TO REAL DATA\n",
    "# ==========================================\n",
    "print(\"\uD83D\uDE80 Running the consolidation on the real database...\\n\")\n",
    "\n",
    "df_orders = spark.table(\"default.silver_orders\")\n",
    "\n",
    "# This is the df_3 requested by the exercise\n",
    "df_3 = get_companies_with_salesowners(df_orders)\n",
    "\n",
    "# Visualise the result\n",
    "display(df_3.limit(10))\n",
    "\n",
    "# ==========================================\n",
    "# 3. APPLICATION TO REAL DATA (Persisting Gold)\n",
    "# ==========================================\n",
    "print(\"\uD83D\uDE80 Running the consolidation and saving to Gold Layer...\\n\")\n",
    "\n",
    "# Reading from Silver as required for Gold logic\n",
    "df_orders = spark.table(\"default.silver_orders\")\n",
    "\n",
    "# Calculate the final business logic\n",
    "df_gold_companies_owners = get_companies_with_salesowners(df_orders)\n",
    "\n",
    "df_gold_companies_owners.write.format(\"delta\").mode(\"overwrite\") \\\n",
    "    .saveAsTable(\"default.gold_companies_salesowners\")\n",
    "\n",
    "print(\"\uD83C\uDFC6 Table 'default.gold_companies_salesowners' created successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9443d1ca-85ee-4c36-b851-19b35c2e9f06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# \uD83D\uDCCA 4. Consumption and Visualisation (Test 6)\n",
    "**Objective:** Develop an interactive control panel (Dashboard) for sales team *stakeholders* to analyse sales representative performance and the distribution of crate types.\n",
    "\n",
    "To meet the highest Data Engineering standards and fulfil the challenge requirement of providing a **reproducible execution environment**, the visualisation solution is decoupled from this processing notebook following this architecture:\n",
    "\n",
    "1. **Data Extraction:** We will export a *snapshot* of the clean data from our Databricks database to a portable format (CSV/Parquet).\n",
    "2. **Frontend Development:** We will build an interactive analytical web application using **Streamlit** and **Plotly** (outside of Databricks).\n",
    "3. **Containerisation (Docker):** We will package the web application and its dependencies in a **Docker** container. This ensures that any reviewer can launch the dashboard on their local machine with a single command, without worrying about library versions or environment configurations."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5786201372549849,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "IFCO Data Engineering Challenge",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}